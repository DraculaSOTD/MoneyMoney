"""
Temporal Graph Neural Network for Dynamic On-Chain Analysis.

Implements temporal GNN for analyzing evolving blockchain transaction patterns.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))
from utils.matrix_operations import MatrixOperations


@dataclass
class TemporalSnapshot:
    """Represents a temporal snapshot of the graph."""
    timestamp: int
    node_features: np.ndarray
    edge_index: np.ndarray
    edge_features: np.ndarray
    adjacency_matrix: np.ndarray
    graph_stats: Dict


class TemporalGraphNetwork:
    """
    Temporal Graph Neural Network for dynamic graph analysis.
    
    Features:
    - Multi-snapshot graph learning
    - Temporal attention mechanisms
    - Dynamic node embedding evolution
    - Time-aware graph convolutions
    - Trend detection in graph structure
    """
    
    def __init__(self,
                 node_feature_dim: int,
                 edge_feature_dim: int = 6,
                 hidden_dim: int = 64,
                 temporal_dim: int = 32,
                 num_snapshots: int = 24,
                 snapshot_interval: int = 3600,  # 1 hour
                 attention_heads: int = 4,
                 num_layers: int = 3):
        """
        Initialize Temporal GNN.
        
        Args:
            node_feature_dim: Dimension of node features
            edge_feature_dim: Dimension of edge features
            hidden_dim: Hidden layer dimension
            temporal_dim: Temporal embedding dimension
            num_snapshots: Number of temporal snapshots to consider
            snapshot_interval: Time interval between snapshots (seconds)
            attention_heads: Number of attention heads
            num_layers: Number of GNN layers
        """
        self.node_feature_dim = node_feature_dim
        self.edge_feature_dim = edge_feature_dim
        self.hidden_dim = hidden_dim
        self.temporal_dim = temporal_dim
        self.num_snapshots = num_snapshots
        self.snapshot_interval = snapshot_interval
        self.attention_heads = attention_heads
        self.num_layers = num_layers
        
        # Initialize components
        self._initialize_parameters()
        
        # Temporal graph storage
        self.snapshots: List[TemporalSnapshot] = []
        self.node_evolution_cache = {}
        
        # Training state
        self.training = True
        
    def _initialize_parameters(self):
        """Initialize all model parameters."""
        # Temporal encoding
        self.temporal_encoder = TemporalEncoder(
            input_dim=1,  # timestamp
            output_dim=self.temporal_dim
        )
        
        # Node encoder
        self.node_encoder = NodeEncoder(
            input_dim=self.node_feature_dim + self.temporal_dim,
            hidden_dim=self.hidden_dim,
            output_dim=self.hidden_dim
        )
        
        # Edge encoder
        self.edge_encoder = EdgeEncoder(
            input_dim=self.edge_feature_dim + self.temporal_dim,
            hidden_dim=self.hidden_dim,
            output_dim=self.hidden_dim
        )
        
        # Temporal GNN layers
        self.tgnn_layers = []\n        for i in range(self.num_layers):\n            layer = TemporalGNNLayer(\n                node_dim=self.hidden_dim,\n                edge_dim=self.hidden_dim,\n                hidden_dim=self.hidden_dim,\n                attention_heads=self.attention_heads\n            )\n            self.tgnn_layers.append(layer)\n            \n        # Temporal attention\n        self.temporal_attention = TemporalAttention(\n            input_dim=self.hidden_dim,\n            num_snapshots=self.num_snapshots,\n            attention_heads=self.attention_heads\n        )\n        \n        # Output layers\n        self.graph_classifier = GraphClassifier(\n            input_dim=self.hidden_dim,\n            hidden_dim=self.hidden_dim,\n            num_classes=3  # sell, hold, buy\n        )\n        \n        self.trend_predictor = TrendPredictor(\n            input_dim=self.hidden_dim,\n            hidden_dim=self.hidden_dim,\n            prediction_steps=6  # 6 hours ahead\n        )\n        \n    def add_snapshot(self, snapshot: TemporalSnapshot):\n        \"\"\"Add a new temporal snapshot.\"\"\"\n        self.snapshots.append(snapshot)\n        \n        # Keep only recent snapshots\n        if len(self.snapshots) > self.num_snapshots:\n            self.snapshots = self.snapshots[-self.num_snapshots:]\n            \n        # Update node evolution cache\n        self._update_node_evolution(snapshot)\n        \n    def _update_node_evolution(self, snapshot: TemporalSnapshot):\n        \"\"\"Update node evolution tracking.\"\"\"\n        timestamp = snapshot.timestamp\n        n_nodes = snapshot.node_features.shape[0]\n        \n        for node_id in range(n_nodes):\n            if node_id not in self.node_evolution_cache:\n                self.node_evolution_cache[node_id] = {\n                    'timestamps': [],\n                    'features': [],\n                    'centrality_scores': [],\n                    'activity_levels': []\n                }\n                \n            cache = self.node_evolution_cache[node_id]\n            cache['timestamps'].append(timestamp)\n            cache['features'].append(snapshot.node_features[node_id])\n            \n            # Compute centrality (degree centrality)\n            degree = np.sum(snapshot.adjacency_matrix[node_id] > 0)\n            cache['centrality_scores'].append(degree)\n            \n            # Compute activity level (sum of edge weights)\n            activity = np.sum(snapshot.adjacency_matrix[node_id])\n            cache['activity_levels'].append(activity)\n            \n            # Keep recent history only\n            max_history = self.num_snapshots\n            for key in ['timestamps', 'features', 'centrality_scores', 'activity_levels']:\n                if len(cache[key]) > max_history:\n                    cache[key] = cache[key][-max_history:]\n                    \n    def forward(self, target_timestamp: Optional[int] = None) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Forward pass through temporal GNN.\n        \n        Args:\n            target_timestamp: Timestamp for prediction (default: latest)\n            \n        Returns:\n            Predictions and embeddings\n        \"\"\"\n        if len(self.snapshots) == 0:\n            raise ValueError(\"No snapshots available\")\n            \n        if target_timestamp is None:\n            target_timestamp = self.snapshots[-1].timestamp\n            \n        # Process each snapshot\n        snapshot_embeddings = []\n        temporal_encodings = []\n        \n        for snapshot in self.snapshots:\n            # Encode temporal information\n            time_diff = (target_timestamp - snapshot.timestamp) / 3600.0  # Hours\n            temporal_encoding = self.temporal_encoder.forward(\n                np.array([[time_diff]])\n            )\n            temporal_encodings.append(temporal_encoding)\n            \n            # Process snapshot\n            snapshot_embedding = self._process_snapshot(\n                snapshot, temporal_encoding\n            )\n            snapshot_embeddings.append(snapshot_embedding)\n            \n        # Apply temporal attention\n        attended_embedding = self.temporal_attention.forward(\n            snapshot_embeddings, temporal_encodings\n        )\n        \n        # Generate predictions\n        graph_prediction = self.graph_classifier.forward(attended_embedding)\n        trend_prediction = self.trend_predictor.forward(attended_embedding)\n        \n        return {\n            'graph_prediction': graph_prediction,\n            'trend_prediction': trend_prediction,\n            'graph_embedding': attended_embedding,\n            'snapshot_embeddings': snapshot_embeddings\n        }\n    \n    def _process_snapshot(self, \n                         snapshot: TemporalSnapshot,\n                         temporal_encoding: np.ndarray) -> np.ndarray:\n        \"\"\"Process a single temporal snapshot.\"\"\"\n        # Expand temporal encoding to match nodes\n        n_nodes = snapshot.node_features.shape[0]\n        temporal_expanded = np.tile(temporal_encoding, (n_nodes, 1))\n        \n        # Encode nodes with temporal information\n        node_input = np.concatenate([\n            snapshot.node_features,\n            temporal_expanded\n        ], axis=1)\n        \n        node_embeddings = self.node_encoder.forward(node_input)\n        \n        # Encode edges with temporal information (if edges exist)\n        if snapshot.edge_features.shape[0] > 0:\n            n_edges = snapshot.edge_features.shape[0]\n            edge_temporal = np.tile(temporal_encoding, (n_edges, 1))\n            \n            edge_input = np.concatenate([\n                snapshot.edge_features,\n                edge_temporal\n            ], axis=1)\n            \n            edge_embeddings = self.edge_encoder.forward(edge_input)\n        else:\n            edge_embeddings = np.array([]).reshape(0, self.hidden_dim)\n            \n        # Apply temporal GNN layers\n        h_nodes = node_embeddings\n        h_edges = edge_embeddings\n        \n        for layer in self.tgnn_layers:\n            h_nodes, h_edges = layer.forward(\n                h_nodes, h_edges, snapshot.edge_index, snapshot.adjacency_matrix\n            )\n            \n        # Graph-level pooling\n        graph_embedding = self._pool_graph(h_nodes, snapshot.adjacency_matrix)\n        \n        return graph_embedding\n    \n    def _pool_graph(self, \n                   node_embeddings: np.ndarray,\n                   adjacency_matrix: np.ndarray) -> np.ndarray:\n        \"\"\"Pool node embeddings to graph representation.\"\"\"\n        if node_embeddings.shape[0] == 0:\n            return np.zeros(self.hidden_dim)\n            \n        # Weighted pooling based on node centrality\n        centrality = np.sum(adjacency_matrix, axis=1) + np.sum(adjacency_matrix, axis=0)\n        centrality = centrality / (np.sum(centrality) + 1e-8)\n        \n        # Weighted average\n        graph_embedding = np.average(node_embeddings, axis=0, weights=centrality)\n        \n        return graph_embedding\n    \n    def predict_trading_signal(self, \n                              current_timestamp: Optional[int] = None) -> Dict[str, float]:\n        \"\"\"Predict trading signal from temporal graph analysis.\"\"\"\n        self.training = False\n        \n        try:\n            result = self.forward(current_timestamp)\n            \n            # Extract graph prediction\n            graph_pred = result['graph_prediction']\n            graph_probs = self._softmax(graph_pred.reshape(1, -1))[0]\n            \n            # Extract trend prediction\n            trend_pred = result['trend_prediction']\n            \n            # Map to trading actions\n            signal_mapping = ['sell', 'hold', 'buy']\n            predicted_action = signal_mapping[np.argmax(graph_probs)]\n            confidence = np.max(graph_probs)\n            \n            # Analyze trend\n            trend_strength = np.mean(np.abs(trend_pred))\n            trend_direction = 'bullish' if np.mean(trend_pred) > 0 else 'bearish'\n            \n            return {\n                'action': predicted_action,\n                'confidence': confidence,\n                'action_probabilities': {\n                    'sell': graph_probs[0],\n                    'hold': graph_probs[1],\n                    'buy': graph_probs[2]\n                },\n                'trend_strength': trend_strength,\n                'trend_direction': trend_direction,\n                'trend_forecast': trend_pred.tolist(),\n                'graph_embedding': result['graph_embedding']\n            }\n            \n        finally:\n            self.training = True\n    \n    def analyze_temporal_patterns(self) -> Dict[str, np.ndarray]:\n        \"\"\"Analyze temporal patterns in the graph sequence.\"\"\"\n        if len(self.snapshots) < 3:\n            return {}\n            \n        # Extract temporal features\n        timestamps = [s.timestamp for s in self.snapshots]\n        graph_stats = [s.graph_stats for s in self.snapshots]\n        \n        # Analyze graph evolution\n        num_nodes_evolution = [stats.get('num_nodes', 0) for stats in graph_stats]\n        num_edges_evolution = [stats.get('num_edges', 0) for stats in graph_stats]\n        density_evolution = [stats.get('density', 0) for stats in graph_stats]\n        \n        # Compute trends\n        node_trend = self._compute_trend(np.array(num_nodes_evolution))\n        edge_trend = self._compute_trend(np.array(num_edges_evolution))\n        density_trend = self._compute_trend(np.array(density_evolution))\n        \n        # Analyze node evolution patterns\n        node_patterns = self._analyze_node_evolution_patterns()\n        \n        # Detect anomalies\n        anomalies = self._detect_temporal_anomalies()\n        \n        return {\n            'timestamps': np.array(timestamps),\n            'graph_evolution': {\n                'num_nodes': np.array(num_nodes_evolution),\n                'num_edges': np.array(num_edges_evolution),\n                'density': np.array(density_evolution)\n            },\n            'trends': {\n                'node_trend': node_trend,\n                'edge_trend': edge_trend,\n                'density_trend': density_trend\n            },\n            'node_patterns': node_patterns,\n            'anomalies': anomalies\n        }\n    \n    def _compute_trend(self, values: np.ndarray) -> Dict[str, float]:\n        \"\"\"Compute trend statistics for a time series.\"\"\"\n        if len(values) < 2:\n            return {'slope': 0, 'r_squared': 0, 'volatility': 0}\n            \n        # Linear regression\n        x = np.arange(len(values))\n        A = np.vstack([x, np.ones(len(x))]).T\n        slope, intercept = np.linalg.lstsq(A, values, rcond=None)[0]\n        \n        # R-squared\n        y_pred = slope * x + intercept\n        ss_res = np.sum((values - y_pred) ** 2)\n        ss_tot = np.sum((values - np.mean(values)) ** 2)\n        r_squared = 1 - (ss_res / (ss_tot + 1e-8))\n        \n        # Volatility\n        volatility = np.std(values)\n        \n        return {\n            'slope': slope,\n            'r_squared': r_squared,\n            'volatility': volatility\n        }\n    \n    def _analyze_node_evolution_patterns(self) -> Dict:\n        \"\"\"Analyze evolution patterns of individual nodes.\"\"\"\n        patterns = {\n            'persistent_nodes': [],  # Nodes present in most snapshots\n            'emerging_nodes': [],    # Nodes appearing recently\n            'disappearing_nodes': [], # Nodes that disappeared\n            'stable_influencers': [], # Nodes with stable high centrality\n            'rising_influencers': []  # Nodes with increasing centrality\n        }\n        \n        if not self.node_evolution_cache:\n            return patterns\n            \n        current_time = self.snapshots[-1].timestamp if self.snapshots else 0\n        recent_threshold = current_time - (self.snapshot_interval * 6)  # Last 6 snapshots\n        \n        for node_id, evolution in self.node_evolution_cache.items():\n            timestamps = evolution['timestamps']\n            centrality_scores = evolution['centrality_scores']\n            \n            if len(timestamps) < 2:\n                continue\n                \n            # Persistence analysis\n            presence_ratio = len(timestamps) / len(self.snapshots)\n            if presence_ratio > 0.8:\n                patterns['persistent_nodes'].append(node_id)\n                \n            # Emergence analysis\n            first_appearance = min(timestamps)\n            if first_appearance > recent_threshold:\n                patterns['emerging_nodes'].append(node_id)\n                \n            # Disappearance analysis\n            last_appearance = max(timestamps)\n            if last_appearance < current_time - (self.snapshot_interval * 3):\n                patterns['disappearing_nodes'].append(node_id)\n                \n            # Influence analysis\n            if len(centrality_scores) >= 3:\n                recent_centrality = np.mean(centrality_scores[-3:])\n                centrality_trend = self._compute_trend(np.array(centrality_scores))\n                \n                if recent_centrality > np.percentile(\n                    [np.mean(evo['centrality_scores'][-3:]) if len(evo['centrality_scores']) >= 3 else 0\n                     for evo in self.node_evolution_cache.values()], 80\n                ):\n                    if abs(centrality_trend['slope']) < 0.1:  # Stable\n                        patterns['stable_influencers'].append(node_id)\n                    elif centrality_trend['slope'] > 0.1:  # Rising\n                        patterns['rising_influencers'].append(node_id)\n                        \n        return patterns\n    \n    def _detect_temporal_anomalies(self) -> List[Dict]:\n        \"\"\"Detect anomalies in temporal graph patterns.\"\"\"\n        anomalies = []\n        \n        if len(self.snapshots) < 5:\n            return anomalies\n            \n        # Extract metrics for anomaly detection\n        metrics = []\n        for snapshot in self.snapshots:\n            stats = snapshot.graph_stats\n            metrics.append([\n                stats.get('num_nodes', 0),\n                stats.get('num_edges', 0),\n                stats.get('density', 0),\n                stats.get('avg_degree', 0),\n                stats.get('clustering_coefficient', 0)\n            ])\n            \n        metrics = np.array(metrics)\n        \n        # Simple anomaly detection using z-score\n        for i, snapshot in enumerate(self.snapshots):\n            snapshot_metrics = metrics[i]\n            \n            # Use sliding window for local anomaly detection\n            window_start = max(0, i - 4)\n            window_end = min(len(metrics), i + 5)\n            window_metrics = metrics[window_start:window_end]\n            \n            # Compute z-scores\n            mean_vals = np.mean(window_metrics, axis=0)\n            std_vals = np.std(window_metrics, axis=0)\n            z_scores = np.abs((snapshot_metrics - mean_vals) / (std_vals + 1e-8))\n            \n            # Detect anomalies (z-score > 2.5)\n            anomaly_indices = np.where(z_scores > 2.5)[0]\n            \n            if len(anomaly_indices) > 0:\n                metric_names = ['num_nodes', 'num_edges', 'density', 'avg_degree', 'clustering_coeff']\n                anomalous_metrics = [metric_names[idx] for idx in anomaly_indices]\n                \n                anomalies.append({\n                    'timestamp': snapshot.timestamp,\n                    'snapshot_index': i,\n                    'anomalous_metrics': anomalous_metrics,\n                    'z_scores': z_scores[anomaly_indices].tolist(),\n                    'severity': np.max(z_scores)\n                })\n                \n        return anomalies\n    \n    def _softmax(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Softmax with numerical stability.\"\"\"\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n    \n    def get_temporal_embeddings(self) -> Dict[str, np.ndarray]:\n        \"\"\"Get temporal embeddings for all snapshots.\"\"\"\n        self.training = False\n        \n        try:\n            result = self.forward()\n            return {\n                'snapshot_embeddings': np.array(result['snapshot_embeddings']),\n                'graph_embedding': result['graph_embedding']\n            }\n        finally:\n            self.training = True\n\n\nclass TemporalEncoder:\n    \"\"\"Encodes temporal information.\"\"\"\n    \n    def __init__(self, input_dim: int, output_dim: int):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        \n        # Positional encoding parameters\n        self.W = self._xavier_init((output_dim, input_dim))\n        self.b = np.zeros((output_dim, 1))\n        \n    def _xavier_init(self, shape: Tuple[int, int]) -> np.ndarray:\n        fan_in, fan_out = shape[1], shape[0]\n        limit = np.sqrt(6 / (fan_in + fan_out))\n        return np.random.uniform(-limit, limit, shape)\n    \n    def forward(self, timestamps: np.ndarray) -> np.ndarray:\n        \"\"\"Encode timestamps to temporal embeddings.\"\"\"\n        # Simple linear transformation with sinusoidal encoding\n        x = timestamps.T  # (input_dim, batch_size)\n        \n        # Linear transformation\n        linear_encoding = self.W @ x + self.b\n        \n        # Add sinusoidal encoding\n        position = timestamps.flatten()\n        sin_encoding = np.sin(position / 10000.0 ** (2 * np.arange(self.output_dim // 2) / self.output_dim))\n        cos_encoding = np.cos(position / 10000.0 ** (2 * np.arange(self.output_dim // 2) / self.output_dim))\n        \n        # Interleave sin and cos\n        sinusoidal = np.zeros((len(position), self.output_dim))\n        sinusoidal[:, 0::2] = sin_encoding[:, :self.output_dim // 2]\n        sinusoidal[:, 1::2] = cos_encoding[:, :self.output_dim // 2]\n        \n        # Combine encodings\n        combined = linear_encoding.T + sinusoidal\n        \n        return combined\n\n\nclass NodeEncoder:\n    \"\"\"Encodes node features.\"\"\"\n    \n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n        self.W1 = self._xavier_init((hidden_dim, input_dim))\n        self.b1 = np.zeros((hidden_dim, 1))\n        self.W2 = self._xavier_init((output_dim, hidden_dim))\n        self.b2 = np.zeros((output_dim, 1))\n        \n    def _xavier_init(self, shape: Tuple[int, int]) -> np.ndarray:\n        fan_in, fan_out = shape[1], shape[0]\n        limit = np.sqrt(6 / (fan_in + fan_out))\n        return np.random.uniform(-limit, limit, shape)\n    \n    def forward(self, x: np.ndarray) -> np.ndarray:\n        x = x.T\n        h = np.maximum(0, self.W1 @ x + self.b1)  # ReLU\n        out = self.W2 @ h + self.b2\n        return out.T\n\n\nclass EdgeEncoder:\n    \"\"\"Encodes edge features.\"\"\"\n    \n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n        self.W1 = self._xavier_init((hidden_dim, input_dim))\n        self.b1 = np.zeros((hidden_dim, 1))\n        self.W2 = self._xavier_init((output_dim, hidden_dim))\n        self.b2 = np.zeros((output_dim, 1))\n        \n    def _xavier_init(self, shape: Tuple[int, int]) -> np.ndarray:\n        fan_in, fan_out = shape[1], shape[0]\n        limit = np.sqrt(6 / (fan_in + fan_out))\n        return np.random.uniform(-limit, limit, shape)\n    \n    def forward(self, x: np.ndarray) -> np.ndarray:\n        if x.shape[0] == 0:\n            return x\n        x = x.T\n        h = np.maximum(0, self.W1 @ x + self.b1)  # ReLU\n        out = self.W2 @ h + self.b2\n        return out.T\n\n\nclass TemporalGNNLayer:\n    \"\"\"Single temporal GNN layer.\"\"\"\n    \n    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int, attention_heads: int = 4):\n        self.node_dim = node_dim\n        self.edge_dim = edge_dim\n        self.hidden_dim = hidden_dim\n        self.attention_heads = attention_heads\n        \n        # Node update parameters\n        self.W_node = self._xavier_init((hidden_dim, node_dim))\n        self.W_msg = self._xavier_init((hidden_dim, node_dim + edge_dim))\n        self.b_node = np.zeros((hidden_dim, 1))\n        \n        # Edge update parameters\n        self.W_edge = self._xavier_init((hidden_dim, edge_dim + 2 * node_dim))\n        self.b_edge = np.zeros((hidden_dim, 1))\n        \n    def _xavier_init(self, shape: Tuple[int, int]) -> np.ndarray:\n        fan_in, fan_out = shape[1], shape[0]\n        limit = np.sqrt(6 / (fan_in + fan_out))\n        return np.random.uniform(-limit, limit, shape)\n    \n    def forward(self, \n                node_features: np.ndarray,\n                edge_features: np.ndarray,\n                edge_index: np.ndarray,\n                adjacency_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Forward pass through temporal GNN layer.\"\"\"\n        n_nodes = node_features.shape[0]\n        \n        # Update edges\n        if edge_features.shape[0] > 0 and edge_index.shape[1] > 0:\n            updated_edges = self._update_edges(node_features, edge_features, edge_index)\n        else:\n            updated_edges = edge_features\n            \n        # Update nodes\n        updated_nodes = self._update_nodes(\n            node_features, updated_edges, edge_index, adjacency_matrix\n        )\n        \n        return updated_nodes, updated_edges\n    \n    def _update_edges(self, \n                     node_features: np.ndarray,\n                     edge_features: np.ndarray,\n                     edge_index: np.ndarray) -> np.ndarray:\n        \"\"\"Update edge features.\"\"\"\n        if edge_index.shape[1] == 0:\n            return edge_features\n            \n        updated_edges = []\n        \n        for i in range(edge_index.shape[1]):\n            src, dst = edge_index[:, i]\n            \n            # Concatenate source node, destination node, and edge features\n            edge_input = np.concatenate([\n                edge_features[i],\n                node_features[src],\n                node_features[dst]\n            ])\n            \n            # Transform\n            updated_edge = self.W_edge @ edge_input.reshape(-1, 1) + self.b_edge\n            updated_edge = np.maximum(0, updated_edge)  # ReLU\n            \n            updated_edges.append(updated_edge.flatten())\n            \n        return np.array(updated_edges)\n    \n    def _update_nodes(self,\n                     node_features: np.ndarray,\n                     edge_features: np.ndarray,\n                     edge_index: np.ndarray,\n                     adjacency_matrix: np.ndarray) -> np.ndarray:\n        \"\"\"Update node features.\"\"\"\n        n_nodes = node_features.shape[0]\n        updated_nodes = []\n        \n        for node_id in range(n_nodes):\n            # Collect messages from neighbors\n            messages = []\n            \n            if edge_index.shape[1] > 0:\n                # Find incoming edges\n                incoming_edges = np.where(edge_index[1] == node_id)[0]\n                \n                for edge_idx in incoming_edges:\n                    src_node = edge_index[0, edge_idx]\n                    edge_feat = edge_features[edge_idx]\n                    \n                    # Create message\n                    message_input = np.concatenate([\n                        node_features[src_node],\n                        edge_feat\n                    ])\n                    \n                    message = self.W_msg @ message_input.reshape(-1, 1)\n                    messages.append(message.flatten())\n                    \n            # Aggregate messages\n            if messages:\n                aggregated_message = np.mean(messages, axis=0)\n            else:\n                aggregated_message = np.zeros(self.hidden_dim)\n                \n            # Update node\n            node_input = node_features[node_id]\n            self_message = self.W_node @ node_input.reshape(-1, 1) + self.b_node\n            \n            updated_node = self_message.flatten() + aggregated_message\n            updated_node = np.maximum(0, updated_node)  # ReLU\n            \n            updated_nodes.append(updated_node)\n            \n        return np.array(updated_nodes)\n\n\nclass TemporalAttention:\n    \"\"\"Temporal attention mechanism.\"\"\"\n    \n    def __init__(self, input_dim: int, num_snapshots: int, attention_heads: int = 4):\n        self.input_dim = input_dim\n        self.num_snapshots = num_snapshots\n        self.attention_heads = attention_heads\n        self.head_dim = input_dim // attention_heads\n        \n        # Attention parameters\n        self.W_q = self._xavier_init((input_dim, input_dim))\n        self.W_k = self._xavier_init((input_dim, input_dim))\n        self.W_v = self._xavier_init((input_dim, input_dim))\n        self.W_o = self._xavier_init((input_dim, input_dim))\n        \n    def _xavier_init(self, shape: Tuple[int, int]) -> np.ndarray:\n        fan_in, fan_out = shape[1], shape[0]\n        limit = np.sqrt(6 / (fan_in + fan_out))\n        return np.random.uniform(-limit, limit, shape)\n    \n    def forward(self, \n                snapshot_embeddings: List[np.ndarray],\n                temporal_encodings: List[np.ndarray]) -> np.ndarray:\n        \"\"\"Apply temporal attention.\"\"\"\n        if not snapshot_embeddings:\n            return np.zeros(self.input_dim)\n            \n        # Stack embeddings\n        embeddings = np.stack(snapshot_embeddings)  # (num_snapshots, input_dim)\n        \n        # Compute attention\n        Q = embeddings @ self.W_q.T\n        K = embeddings @ self.W_k.T\n        V = embeddings @ self.W_v.T\n        \n        # Compute attention scores\n        scores = Q @ K.T / np.sqrt(self.input_dim)\n        \n        # Apply softmax\n        attention_weights = self._softmax(scores)\n        \n        # Apply attention to values\n        attended = attention_weights @ V\n        \n        # Use only the last (most recent) attended representation\n        output = attended[-1] @ self.W_o.T\n        \n        return output\n    \n    def _softmax(self, x: np.ndarray) -> np.ndarray:\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\n\nclass GraphClassifier:\n    \"\"\"Graph-level classifier.\"\"\"\n    \n    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n        self.W1 = self._xavier_init((hidden_dim, input_dim))\n        self.b1 = np.zeros((hidden_dim, 1))\n        self.W2 = self._xavier_init((num_classes, hidden_dim))\n        self.b2 = np.zeros((num_classes, 1))\n        \n    def _xavier_init(self, shape: Tuple[int, int]) -> np.ndarray:\n        fan_in, fan_out = shape[1], shape[0]\n        limit = np.sqrt(6 / (fan_in + fan_out))\n        return np.random.uniform(-limit, limit, shape)\n    \n    def forward(self, x: np.ndarray) -> np.ndarray:\n        x = x.reshape(-1, 1)\n        h = np.maximum(0, self.W1 @ x + self.b1)  # ReLU\n        out = self.W2 @ h + self.b2\n        return out.flatten()\n\n\nclass TrendPredictor:\n    \"\"\"Trend prediction module.\"\"\"\n    \n    def __init__(self, input_dim: int, hidden_dim: int, prediction_steps: int):\n        self.prediction_steps = prediction_steps\n        \n        self.W1 = self._xavier_init((hidden_dim, input_dim))\n        self.b1 = np.zeros((hidden_dim, 1))\n        self.W2 = self._xavier_init((prediction_steps, hidden_dim))\n        self.b2 = np.zeros((prediction_steps, 1))\n        \n    def _xavier_init(self, shape: Tuple[int, int]) -> np.ndarray:\n        fan_in, fan_out = shape[1], shape[0]\n        limit = np.sqrt(6 / (fan_in + fan_out))\n        return np.random.uniform(-limit, limit, shape)\n    \n    def forward(self, x: np.ndarray) -> np.ndarray:\n        x = x.reshape(-1, 1)\n        h = np.maximum(0, self.W1 @ x + self.b1)  # ReLU\n        out = self.W2 @ h + self.b2\n        return out.flatten()"