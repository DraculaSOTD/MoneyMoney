"""
Real-time Anomaly Monitoring System.

Provides real-time anomaly detection and monitoring capabilities for trading systems.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Callable, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque, defaultdict
import threading
import time
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

from models.advanced.anomaly_detection.ensemble_detector import EnsembleAnomalyDetector, EnsembleAnomalyResult


@dataclass
class AnomalyAlert:
    """Real-time anomaly alert."""
    timestamp: datetime
    severity: str
    anomaly_type: str
    description: str
    data_point: Any
    anomaly_score: float
    confidence: float
    affected_features: List[str]
    recommended_actions: List[str]
    metadata: Dict = field(default_factory=dict)


@dataclass
class MonitoringMetrics:
    """Monitoring system metrics."""
    total_data_points: int = 0
    anomalies_detected: int = 0
    false_positives: int = 0
    true_positives: int = 0
    detection_rate: float = 0.0
    false_positive_rate: float = 0.0
    avg_processing_time: float = 0.0
    last_update: datetime = field(default_factory=datetime.now)


class RealtimeAnomalyMonitor:
    """
    Real-time anomaly monitoring system.
    
    Features:
    - Continuous stream processing
    - Multiple data stream monitoring
    - Adaptive thresholds
    - Alert system with severity levels
    - Performance monitoring
    - Historical anomaly tracking
    - Configurable response actions
    """
    
    def __init__(self,
                 ensemble_detector: Optional[EnsembleAnomalyDetector] = None,
                 buffer_size: int = 1000,
                 alert_callback: Optional[Callable] = None,
                 adaptation_window: int = 100,
                 min_data_points: int = 50):
        """
        Initialize real-time anomaly monitor.
        
        Args:
            ensemble_detector: Anomaly detection ensemble
            buffer_size: Size of data buffer for analysis
            alert_callback: Callback function for alerts
            adaptation_window: Window size for adaptive thresholds
            min_data_points: Minimum data points before detection
        """
        # Initialize or create ensemble detector
        if ensemble_detector is None:
            self.detector = EnsembleAnomalyDetector()
        else:
            self.detector = ensemble_detector
            
        self.buffer_size = buffer_size
        self.alert_callback = alert_callback
        self.adaptation_window = adaptation_window
        self.min_data_points = min_data_points
        
        # Data streams and buffers
        self.data_streams: Dict[str, deque] = defaultdict(lambda: deque(maxlen=buffer_size))
        self.feature_names: Dict[str, List[str]] = {}
        
        # Real-time monitoring state
        self.is_monitoring = False
        self.monitoring_thread = None
        self.stream_locks: Dict[str, threading.Lock] = defaultdict(threading.Lock)
        
        # Anomaly tracking\n        self.recent_anomalies: Dict[str, List[AnomalyAlert]] = defaultdict(list)\n        self.anomaly_history: Dict[str, List[AnomalyAlert]] = defaultdict(list)\n        \n        # Adaptive thresholds\n        self.adaptive_thresholds: Dict[str, float] = defaultdict(float)\n        self.threshold_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=adaptation_window))\n        \n        # Performance metrics\n        self.metrics: Dict[str, MonitoringMetrics] = defaultdict(MonitoringMetrics)\n        \n        # Alert configuration\n        self.alert_config = {\n            'critical': {'cooldown': 60, 'max_per_hour': 10},\n            'high': {'cooldown': 300, 'max_per_hour': 20},\n            'medium': {'cooldown': 600, 'max_per_hour': 50},\n            'low': {'cooldown': 1200, 'max_per_hour': 100}\n        }\n        \n        self.last_alerts: Dict[str, Dict[str, datetime]] = defaultdict(dict)\n        \n    def add_data_stream(self, \n                       stream_name: str,\n                       feature_names: List[str],\n                       initial_data: Optional[np.ndarray] = None):\n        \"\"\"Add a new data stream for monitoring.\"\"\"\n        self.feature_names[stream_name] = feature_names\n        \n        if initial_data is not None:\n            # Initialize buffer with historical data\n            for data_point in initial_data:\n                self.data_streams[stream_name].append(data_point)\n                \n            # Fit detector if enough data\n            if len(self.data_streams[stream_name]) >= self.min_data_points:\n                self._fit_detector_for_stream(stream_name)\n                \n    def _fit_detector_for_stream(self, stream_name: str):\n        \"\"\"Fit detector for a specific stream.\"\"\"\n        data = np.array(list(self.data_streams[stream_name]))\n        \n        if len(data.shape) == 1:\n            data = data.reshape(-1, 1)\n            \n        try:\n            # Clone detector for this stream if not already fitted\n            if not self.detector.is_fitted:\n                self.detector.fit(data)\n            print(f\"Detector fitted for stream {stream_name} with {len(data)} samples\")\n        except Exception as e:\n            print(f\"Error fitting detector for stream {stream_name}: {e}\")\n    \n    def add_data_point(self, \n                      stream_name: str,\n                      data_point: np.ndarray,\n                      timestamp: Optional[datetime] = None) -> Optional[AnomalyAlert]:\n        \"\"\"Add a single data point and check for anomalies.\"\"\"\n        if timestamp is None:\n            timestamp = datetime.now()\n            \n        with self.stream_locks[stream_name]:\n            # Add to buffer\n            self.data_streams[stream_name].append(data_point)\n            \n            # Update metrics\n            self.metrics[stream_name].total_data_points += 1\n            \n            # Check if we have enough data for detection\n            if len(self.data_streams[stream_name]) < self.min_data_points:\n                return None\n                \n            # Ensure detector is fitted\n            if not self.detector.is_fitted:\n                self._fit_detector_for_stream(stream_name)\n                \n            # Detect anomalies\n            start_time = time.time()\n            alert = self._detect_anomaly_single(stream_name, data_point, timestamp)\n            processing_time = time.time() - start_time\n            \n            # Update performance metrics\n            self._update_performance_metrics(stream_name, processing_time, alert)\n            \n            # Handle alert\n            if alert:\n                self._handle_alert(stream_name, alert)\n                \n            return alert\n    \n    def _detect_anomaly_single(self, \n                              stream_name: str,\n                              data_point: np.ndarray,\n                              timestamp: datetime) -> Optional[AnomalyAlert]:\n        \"\"\"Detect anomaly for a single data point.\"\"\"\n        try:\n            # Prepare data for detection\n            data_point = np.array(data_point).reshape(1, -1)\n            \n            # Get anomaly detection result\n            results = self.detector.detect_anomalies(data_point)\n            \n            if not results:\n                return None\n                \n            result = results[0]\n            \n            # Check if this qualifies as an alert\n            if not result.is_anomaly:\n                return None\n                \n            # Check adaptive threshold\n            adaptive_threshold = self.adaptive_thresholds.get(stream_name, 0.5)\n            if result.ensemble_score < adaptive_threshold:\n                return None\n                \n            # Create alert\n            alert = self._create_alert(\n                stream_name, result, data_point.flatten(), timestamp\n            )\n            \n            return alert\n            \n        except Exception as e:\n            print(f\"Error detecting anomaly for stream {stream_name}: {e}\")\n            return None\n    \n    def _create_alert(self,\n                     stream_name: str,\n                     result: EnsembleAnomalyResult,\n                     data_point: np.ndarray,\n                     timestamp: datetime) -> AnomalyAlert:\n        \"\"\"Create anomaly alert from detection result.\"\"\"\n        # Determine anomaly type\n        anomaly_type = self._classify_anomaly_type(stream_name, result, data_point)\n        \n        # Generate description\n        description = f\"Anomaly detected in {stream_name}: {result.explanation}\"\n        \n        # Identify affected features\n        affected_features = self._identify_affected_features(\n            stream_name, result, data_point\n        )\n        \n        # Generate recommended actions\n        recommended_actions = self._generate_recommendations(\n            stream_name, result.severity, anomaly_type\n        )\n        \n        alert = AnomalyAlert(\n            timestamp=timestamp,\n            severity=result.severity,\n            anomaly_type=anomaly_type,\n            description=description,\n            data_point=data_point,\n            anomaly_score=result.ensemble_score,\n            confidence=result.confidence,\n            affected_features=affected_features,\n            recommended_actions=recommended_actions,\n            metadata={\n                'stream_name': stream_name,\n                'method_scores': result.method_scores,\n                'consensus_level': result.consensus_level\n            }\n        )\n        \n        return alert\n    \n    def _classify_anomaly_type(self,\n                              stream_name: str,\n                              result: EnsembleAnomalyResult,\n                              data_point: np.ndarray) -> str:\n        \"\"\"Classify the type of anomaly.\"\"\"\n        # Analyze which methods detected the anomaly\n        detecting_methods = [method for method, pred in result.method_predictions.items() if pred]\n        \n        if 'statistical' in detecting_methods and len(detecting_methods) == 1:\n            return 'statistical_outlier'\n        elif 'lof' in detecting_methods and 'isolation_forest' not in detecting_methods:\n            return 'density_anomaly'\n        elif 'isolation_forest' in detecting_methods and 'lof' not in detecting_methods:\n            return 'isolation_anomaly'\n        elif len(detecting_methods) > 1:\n            return 'multi_method_anomaly'\n        else:\n            return 'unknown_anomaly'\n    \n    def _identify_affected_features(self,\n                                   stream_name: str,\n                                   result: EnsembleAnomalyResult,\n                                   data_point: np.ndarray) -> List[str]:\n        \"\"\"Identify which features are most anomalous.\"\"\"\n        feature_names = self.feature_names.get(stream_name, [])\n        \n        if not feature_names or len(feature_names) != len(data_point):\n            return [f\"feature_{i}\" for i in range(len(data_point))]\n            \n        # Simple approach: identify features with extreme values\n        # In practice, this would use feature importance from the detector\n        \n        # Get recent data for comparison\n        recent_data = list(self.data_streams[stream_name])[-20:]  # Last 20 points\n        \n        if len(recent_data) < 2:\n            return feature_names\n            \n        recent_array = np.array(recent_data)\n        if len(recent_array.shape) == 1:\n            recent_array = recent_array.reshape(-1, 1)\n            \n        affected = []\n        \n        for i, (feature_name, value) in enumerate(zip(feature_names, data_point)):\n            if i < recent_array.shape[1]:\n                feature_data = recent_array[:, i]\n                mean_val = np.mean(feature_data)\n                std_val = np.std(feature_data)\n                \n                if std_val > 0:\n                    z_score = abs(value - mean_val) / std_val\n                    if z_score > 2.0:  # Significant deviation\n                        affected.append(feature_name)\n                        \n        return affected if affected else feature_names[:3]  # Return first 3 if none specifically affected\n    \n    def _generate_recommendations(self,\n                                stream_name: str,\n                                severity: str,\n                                anomaly_type: str) -> List[str]:\n        \"\"\"Generate recommended actions based on anomaly.\"\"\"\n        recommendations = []\n        \n        if severity in ['critical', 'high']:\n            recommendations.append(\"Investigate immediately\")\n            recommendations.append(\"Consider stopping automated trading\")\n            \n        if anomaly_type == 'density_anomaly':\n            recommendations.append(\"Check for unusual market conditions\")\n            recommendations.append(\"Verify data feed integrity\")\n        elif anomaly_type == 'isolation_anomaly':\n            recommendations.append(\"Review system performance metrics\")\n            recommendations.append(\"Check for potential data corruption\")\n        elif anomaly_type == 'statistical_outlier':\n            recommendations.append(\"Analyze recent market events\")\n            recommendations.append(\"Consider adjusting risk parameters\")\n            \n        if severity in ['medium', 'low']:\n            recommendations.append(\"Monitor closely for similar patterns\")\n            recommendations.append(\"Log for pattern analysis\")\n            \n        return recommendations\n    \n    def _handle_alert(self, stream_name: str, alert: AnomalyAlert):\n        \"\"\"Handle anomaly alert.\"\"\"\n        # Check alert rate limits\n        if not self._should_send_alert(stream_name, alert.severity):\n            return\n            \n        # Store alert\n        self.recent_anomalies[stream_name].append(alert)\n        self.anomaly_history[stream_name].append(alert)\n        \n        # Keep recent anomalies limited\n        if len(self.recent_anomalies[stream_name]) > 100:\n            self.recent_anomalies[stream_name] = self.recent_anomalies[stream_name][-50:]\n            \n        # Update alert timestamps\n        self.last_alerts[stream_name][alert.severity] = alert.timestamp\n        \n        # Call alert callback if provided\n        if self.alert_callback:\n            try:\n                self.alert_callback(stream_name, alert)\n            except Exception as e:\n                print(f\"Error in alert callback: {e}\")\n                \n        # Update adaptive thresholds\n        self._update_adaptive_threshold(stream_name, alert)\n        \n        print(f\"ANOMALY ALERT [{alert.severity.upper()}] in {stream_name}: {alert.description}\")\n    \n    def _should_send_alert(self, stream_name: str, severity: str) -> bool:\n        \"\"\"Check if alert should be sent based on rate limits.\"\"\"\n        config = self.alert_config.get(severity, {})\n        \n        # Check cooldown\n        cooldown = config.get('cooldown', 0)\n        if cooldown > 0:\n            last_alert = self.last_alerts[stream_name].get(severity)\n            if last_alert:\n                time_since_last = (datetime.now() - last_alert).total_seconds()\n                if time_since_last < cooldown:\n                    return False\n                    \n        # Check hourly limit\n        max_per_hour = config.get('max_per_hour', float('inf'))\n        if max_per_hour < float('inf'):\n            hour_ago = datetime.now() - timedelta(hours=1)\n            recent_alerts = [\n                alert for alert in self.recent_anomalies[stream_name]\n                if alert.timestamp > hour_ago and alert.severity == severity\n            ]\n            if len(recent_alerts) >= max_per_hour:\n                return False\n                \n        return True\n    \n    def _update_adaptive_threshold(self, stream_name: str, alert: AnomalyAlert):\n        \"\"\"Update adaptive threshold based on recent anomalies.\"\"\"\n        # Simple adaptive threshold adjustment\n        current_threshold = self.adaptive_thresholds.get(stream_name, 0.5)\n        \n        # If too many alerts, increase threshold\n        recent_count = len([\n            a for a in self.recent_anomalies[stream_name][-10:]\n            if (datetime.now() - a.timestamp).total_seconds() < 300  # Last 5 minutes\n        ])\n        \n        if recent_count > 5:  # Too many recent alerts\n            new_threshold = min(0.9, current_threshold * 1.1)\n        elif recent_count == 0:  # No recent alerts\n            new_threshold = max(0.1, current_threshold * 0.95)\n        else:\n            new_threshold = current_threshold\n            \n        self.adaptive_thresholds[stream_name] = new_threshold\n        self.threshold_history[stream_name].append(new_threshold)\n    \n    def _update_performance_metrics(self,\n                                  stream_name: str,\n                                  processing_time: float,\n                                  alert: Optional[AnomalyAlert]):\n        \"\"\"Update performance metrics.\"\"\"\n        metrics = self.metrics[stream_name]\n        \n        # Update processing time (moving average)\n        if metrics.avg_processing_time == 0:\n            metrics.avg_processing_time = processing_time\n        else:\n            metrics.avg_processing_time = 0.9 * metrics.avg_processing_time + 0.1 * processing_time\n            \n        # Update detection counts\n        if alert:\n            metrics.anomalies_detected += 1\n            \n        # Update detection rate\n        if metrics.total_data_points > 0:\n            metrics.detection_rate = metrics.anomalies_detected / metrics.total_data_points\n            \n        metrics.last_update = datetime.now()\n    \n    def start_monitoring(self, check_interval: float = 1.0):\n        \"\"\"Start continuous monitoring thread.\"\"\"\n        if self.is_monitoring:\n            return\n            \n        self.is_monitoring = True\n        self.monitoring_thread = threading.Thread(\n            target=self._monitoring_loop,\n            args=(check_interval,),\n            daemon=True\n        )\n        self.monitoring_thread.start()\n        print(\"Real-time anomaly monitoring started\")\n    \n    def stop_monitoring(self):\n        \"\"\"Stop continuous monitoring.\"\"\"\n        self.is_monitoring = False\n        if self.monitoring_thread:\n            self.monitoring_thread.join(timeout=5)\n        print(\"Real-time anomaly monitoring stopped\")\n    \n    def _monitoring_loop(self, check_interval: float):\n        \"\"\"Main monitoring loop.\"\"\"\n        while self.is_monitoring:\n            try:\n                # Perform periodic maintenance\n                self._cleanup_old_data()\n                self._update_global_metrics()\n                \n                time.sleep(check_interval)\n                \n            except Exception as e:\n                print(f\"Error in monitoring loop: {e}\")\n                time.sleep(check_interval)\n    \n    def _cleanup_old_data(self):\n        \"\"\"Clean up old anomaly data.\"\"\"\n        cutoff_time = datetime.now() - timedelta(hours=24)\n        \n        for stream_name in self.recent_anomalies:\n            self.recent_anomalies[stream_name] = [\n                alert for alert in self.recent_anomalies[stream_name]\n                if alert.timestamp > cutoff_time\n            ]\n    \n    def _update_global_metrics(self):\n        \"\"\"Update global monitoring metrics.\"\"\"\n        # This could include system-wide metrics aggregation\n        pass\n    \n    def get_stream_status(self, stream_name: str) -> Dict:\n        \"\"\"Get current status of a data stream.\"\"\"\n        metrics = self.metrics[stream_name]\n        recent_anomalies = self.recent_anomalies[stream_name][-10:]  # Last 10\n        \n        status = {\n            'stream_name': stream_name,\n            'is_active': len(self.data_streams[stream_name]) > 0,\n            'buffer_size': len(self.data_streams[stream_name]),\n            'max_buffer_size': self.data_streams[stream_name].maxlen,\n            'total_data_points': metrics.total_data_points,\n            'anomalies_detected': metrics.anomalies_detected,\n            'detection_rate': metrics.detection_rate,\n            'avg_processing_time': metrics.avg_processing_time,\n            'adaptive_threshold': self.adaptive_thresholds.get(stream_name, 0.5),\n            'recent_anomalies_count': len(recent_anomalies),\n            'last_update': metrics.last_update.isoformat() if metrics.last_update else None\n        }\n        \n        # Add recent anomaly summary\n        if recent_anomalies:\n            status['recent_anomalies'] = [\n                {\n                    'timestamp': alert.timestamp.isoformat(),\n                    'severity': alert.severity,\n                    'type': alert.anomaly_type,\n                    'score': alert.anomaly_score\n                }\n                for alert in recent_anomalies\n            ]\n            \n        return status\n    \n    def get_all_streams_status(self) -> Dict[str, Dict]:\n        \"\"\"Get status of all monitored streams.\"\"\"\n        return {\n            stream_name: self.get_stream_status(stream_name)\n            for stream_name in self.data_streams.keys()\n        }\n    \n    def get_anomaly_summary(self, \n                           stream_name: Optional[str] = None,\n                           hours_back: int = 24) -> Dict:\n        \"\"\"Get summary of anomalies in the specified time window.\"\"\"\n        cutoff_time = datetime.now() - timedelta(hours=hours_back)\n        \n        if stream_name:\n            streams_to_check = [stream_name]\n        else:\n            streams_to_check = list(self.anomaly_history.keys())\n            \n        summary = {\n            'time_window_hours': hours_back,\n            'streams_analyzed': len(streams_to_check),\n            'total_anomalies': 0,\n            'by_severity': defaultdict(int),\n            'by_type': defaultdict(int),\n            'by_stream': {}\n        }\n        \n        for stream in streams_to_check:\n            stream_anomalies = [\n                alert for alert in self.anomaly_history[stream]\n                if alert.timestamp > cutoff_time\n            ]\n            \n            summary['total_anomalies'] += len(stream_anomalies)\n            summary['by_stream'][stream] = len(stream_anomalies)\n            \n            for alert in stream_anomalies:\n                summary['by_severity'][alert.severity] += 1\n                summary['by_type'][alert.anomaly_type] += 1\n                \n        # Convert defaultdicts to regular dicts\n        summary['by_severity'] = dict(summary['by_severity'])\n        summary['by_type'] = dict(summary['by_type'])\n        \n        return summary\n    \n    def configure_alerts(self, \n                        severity: str,\n                        cooldown: Optional[int] = None,\n                        max_per_hour: Optional[int] = None):\n        \"\"\"Configure alert settings for a severity level.\"\"\"\n        if severity not in self.alert_config:\n            self.alert_config[severity] = {}\n            \n        if cooldown is not None:\n            self.alert_config[severity]['cooldown'] = cooldown\n            \n        if max_per_hour is not None:\n            self.alert_config[severity]['max_per_hour'] = max_per_hour\n    \n    def set_alert_callback(self, callback: Callable[[str, AnomalyAlert], None]):\n        \"\"\"Set callback function for anomaly alerts.\"\"\"\n        self.alert_callback = callback\n    \n    def reset_stream(self, stream_name: str):\n        \"\"\"Reset a data stream (clear buffers and history).\"\"\"\n        if stream_name in self.data_streams:\n            self.data_streams[stream_name].clear()\n            \n        if stream_name in self.recent_anomalies:\n            self.recent_anomalies[stream_name].clear()\n            \n        if stream_name in self.anomaly_history:\n            self.anomaly_history[stream_name].clear()\n            \n        # Reset metrics\n        self.metrics[stream_name] = MonitoringMetrics()\n        \n        # Reset adaptive threshold\n        self.adaptive_thresholds[stream_name] = 0.5\n        \n        print(f\"Stream {stream_name} has been reset\")"